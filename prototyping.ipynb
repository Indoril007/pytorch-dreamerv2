{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ffa8e9-5faa-4f1a-b0a5-6da0b5dff5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import gym\n",
    "import safety_gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "from garage.envs import GymEnv\n",
    "\n",
    "sys.path.append('../robust_rewards_from_preferences')\n",
    "import envs.custom_safety_envs\n",
    "del sys.path[-1]\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "from garage.trainer import Trainer\n",
    "from dreamer import Dreamer\n",
    "from utils import RandomPolicy\n",
    "from garage.experiment.experiment import ExperimentContext\n",
    "from garage.sampler import RaySampler, LocalSampler, DefaultWorker\n",
    "import gym.envs.atari\n",
    "from garage.sampler.worker_factory import WorkerFactory\n",
    "import threading\n",
    "\n",
    "import torch\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "from dotmap import DotMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13132c8-fff1-4c48-92d5-de443f032f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = YAML()\n",
    "with open('./config.yaml', 'r') as f:\n",
    "    CONFIG = DotMap(yaml.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef3f5bc-5718-4c5c-a92b-05fcd669e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from garage import EpisodeBatch, TimeStepBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "325f35af-fa34-4ccc-9f9b-6ff1f54721c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from garage.envs.wrappers import ClipReward, EpisodicLife,  FireReset, Grayscale,  MaxAndSkip, Noop,  Resize, StackFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a032ceea-75b1-4763-90b6-d6595fc39ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.atari.AtariEnv('breakout', obs_type='image', frameskip=1, repeat_action_probability=0.25, full_action_space=False)\n",
    "env = Noop(env, noop_max=30)\n",
    "env = MaxAndSkip(env, skip=4)\n",
    "# env = EpisodicLife(env)\n",
    "if CONFIG.image.color_channels == 1:\n",
    "    env = Grayscale(env)\n",
    "env = Resize(env, CONFIG.image.height, CONFIG.image.height)\n",
    "max_episode_length = 108000 / 4\n",
    "env = GymEnv(env, max_episode_length=max_episode_length, is_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2ba1c5-2d6b-4a71-a2ae-78e103e2984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-06 18:34:16,105\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "buf = ReplayBuffer(env.spec)\n",
    "random_policy = RandomPolicy(env.spec)\n",
    "sampler = RaySampler(agents=random_policy, envs=env, max_episode_length=env.spec.max_episode_length, n_workers=2)\n",
    "dreamer = Dreamer(env.spec, sampler=sampler, rssm_model=None, actor=None, critic=None, buf=buf)\n",
    "ctxt = ExperimentContext(snapshot_dir='./snapshot_dir', snapshot_mode='gap_overwrite', snapshot_gap=50)\n",
    "trainer = Trainer(ctxt)\n",
    "trainer.setup(algo=dreamer, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22a10c3d-ec11-4148-8df7-01bc4debe101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesborg/.pyenv/versions/3.8.7/envs/garage_2021.03/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "dreamer._initialize_dataset(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bad397d-fc91-40db-a961-17e41eb15780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch import nn\n",
    "# from torch import distributions\n",
    "# from torch.distributions import kl_divergence, Independent\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def categorical_kl(probs_a, probs_b):\n",
    "#     return torch.sum(probs_a * torch.log(probs_a / probs_b), dim=[-1, -2])\n",
    "\n",
    "# def kl_loss(posterior, prior):\n",
    "#     lhs = categorical_kl(posterior.probs.detach(), prior.probs)\n",
    "#     rhs = categorical_kl(posterior.probs, prior.probs.detach())\n",
    "#     kl_loss = CONFIG.rssm.alpha * lhs + (1 - CONFIG.rssm.alpha) * rhs\n",
    "   \n",
    "#     assert torch.isclose(lhs, rhs).all()\n",
    "    \n",
    "#     expected = kl_divergence(\n",
    "#         Independent(posterior, 1), \n",
    "#         Independent(prior, 1),)\n",
    "    \n",
    "#     assert torch.isclose(lhs, expected, atol=1e-5).all(), lhs - expected\n",
    "    \n",
    "#     return kl_loss\n",
    "\n",
    "# class WorldModel(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, env_spec):\n",
    "#         super().__init__()\n",
    "#         self.env_spec = env_spec\n",
    "        \n",
    "#         self.rssm = RSSM(action_size=self.env_spec.action_space.n)\n",
    "#         self.image_encoder = ImageEncoder()\n",
    "#         self.image_decoder = ImageDecoder()\n",
    "        \n",
    "#         feat_size = (\n",
    "#             CONFIG.rssm.stoch_state_classes * CONFIG.rssm.stoch_state_size\n",
    "#             + CONFIG.rssm.det_state_size\n",
    "#         )\n",
    "        \n",
    "#         self.reward_predictor = MLP(\n",
    "#             input_shape=feat_size,\n",
    "#             units=CONFIG.reward_head.units)\n",
    "        \n",
    "#         self.discount_predictor = MLP(\n",
    "#             input_shape=feat_size,\n",
    "#             units=CONFIG.discount_head.units)\n",
    "        \n",
    "#     def forward(self, observations, actions):\n",
    "#         segs, batch, channels, height, width = observations.shape\n",
    "#         flattened_observations = observations.reshape(segs*batch, channels, height, width)\n",
    "#         embedded_observations = encoder(flattened_observations).reshape(segs, batch, -1)\n",
    "#         out = self.rssm.observe(embedded_observations, actions)\n",
    "#         out['reward_preds'] = self.reward_predictor(out['feats'])\n",
    "#         out['discount_preds'] = self.discount_predictor(out['feats'])\n",
    "#         return out\n",
    "    \n",
    "#     def reward_pred(self, logits):\n",
    "#         dist = torch.distributions.Normal(loc=logits, scale=1)\n",
    "    \n",
    "#     def discount_pred(self, logits):\n",
    "#         dist = torch.distributions.Bernoulli(logits=logits)\n",
    "        \n",
    "# class Actor(object):\n",
    "#     pass\n",
    "\n",
    "# class RSSM(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  action_size,):\n",
    "    \n",
    "#         super().__init__()\n",
    "#         self.action_size = action_size\n",
    "        \n",
    "#         self.embed_size = CONFIG.rssm.embed_size\n",
    "#         self.stoch_state_classes = CONFIG.rssm.stoch_state_classes\n",
    "#         self.stoch_state_size = CONFIG.rssm.stoch_state_size\n",
    "#         self.det_state_size = CONFIG.rssm.det_state_size\n",
    "#         self.act = eval(CONFIG.rssm.act)\n",
    "\n",
    "#         self.register_parameter(\n",
    "#             name='cell_initial_state',\n",
    "#             param=nn.Parameter(torch.zeros(self.det_state_size))\n",
    "#         )\n",
    "\n",
    "#         self.cell = nn.GRUCell(input_size=self.det_state_size,\n",
    "#                                hidden_size=self.det_state_size)\n",
    "\n",
    "#         self._initialize_imagination_layers()\n",
    "#         self._initialize_observation_layers()\n",
    "        \n",
    "#     def _initialize_imagination_layers(self):\n",
    "#         self.embed_stoch_state_and_action = nn.Linear(\n",
    "#             self.action_size + self.stoch_state_size * self.stoch_state_classes,\n",
    "#             self.det_state_size)\n",
    "        \n",
    "#         self.imagine_out_1 = nn.Linear(self.det_state_size, self.det_state_size)\n",
    "#         self.imagine_out_2 = nn.Linear(\n",
    "#             self.det_state_size,\n",
    "#             self.stoch_state_classes*self.stoch_state_size)\n",
    "    \n",
    "#     def _initialize_observation_layers(self):\n",
    "#         self.observe_out_1 = nn.Linear(\n",
    "#             self.det_state_size + self.embed_size,\n",
    "#             self.det_state_size)\n",
    "#         self.observe_out_2 = nn.Linear(\n",
    "#             self.det_state_size,\n",
    "#             self.stoch_state_classes*self.stoch_state_size)\n",
    "    \n",
    "#     def initial_state(self, batch_size):\n",
    "#         state = {\n",
    "#             'logits': torch.zeros(batch_size,\n",
    "#                                   self.stoch_state_size,\n",
    "#                                   self.stoch_state_classes),\n",
    "#             'stoch': torch.zeros(batch_size,\n",
    "#                                  self.stoch_state_size,\n",
    "#                                   self.stoch_state_classes),\n",
    "#             'deter': self.cell_initial_state.repeat([batch_size, 1])\n",
    "#         }\n",
    "#         return state\n",
    "    \n",
    "#     def step(self, prev_stoch, prev_deter, prev_action):\n",
    "#         x = torch.cat((prev_stoch.flatten(start_dim=1), prev_action), dim=-1)\n",
    "#         x = self.act(self.embed_stoch_state_and_action(x))\n",
    "#         deter = self.cell(x, prev_deter)\n",
    "#         return deter\n",
    "    \n",
    "#     def get_stoch(self, x):\n",
    "#         logits = x.reshape(\n",
    "#             *x.shape[:-1],\n",
    "#             self.stoch_state_size,\n",
    "#             self.stoch_state_classes)\n",
    "#         dist = distributions.Categorical(logits=logits)\n",
    "#         sample = F.one_hot(dist.sample(), num_classes=self.stoch_state_classes).type(torch.float)\n",
    "#         sample += dist.probs - dist.probs.detach()  # Straight through gradients trick\n",
    "#         return sample, dist\n",
    "        \n",
    "#     def imagine_step(self, prev_stoch, prev_deter, prev_action):\n",
    "#         deter = self.step(prev_stoch, prev_deter, prev_action)\n",
    "#         x = self.act(self.imagine_out_1(deter))\n",
    "#         x = self.imagine_out_2(x)  \n",
    "#         sample, dist = self.get_stoch(x)\n",
    "#         prior = {'sample': sample, 'dist': dist}\n",
    "#         return prior, deter\n",
    "    \n",
    "#     def observe_step(self, prev_stoch, prev_deter, prev_action, embed):\n",
    "#         prior, deter = self.imagine_step(prev_stoch, prev_deter, prev_action)\n",
    "#         x = torch.cat([deter, embed], dim=-1)\n",
    "#         x = self.act(self.observe_out_1(x))\n",
    "#         x = self.observe_out_2(x)\n",
    "#         sample, dist = self.get_stoch(x)\n",
    "#         posterior = {'sample': sample, 'dist': dist}\n",
    "#         return posterior, prior, deter\n",
    "    \n",
    "#     def imagine(self):\n",
    "#         pass\n",
    "    \n",
    "#     def observe(self, embedded_observations, actions):\n",
    "#         segs, steps, embedding_size = embedded_observations.shape\n",
    "#         assert segs == actions.shape[0]\n",
    "#         assert steps == actions.shape[1]\n",
    "        \n",
    "#         # Change from SEGS x STEPS x N -> STEPS x SEGS x N\n",
    "#         # This facilitates \n",
    "#         embedded_observations = torch.swapaxes(embedded_observations, 0, 1)\n",
    "#         actions = torch.swapaxes(actions, 0, 1)\n",
    "        \n",
    "#         initial = self.initial_state(batch_size=segs)\n",
    "#         stoch, deter = initial['stoch'], initial['deter']\n",
    "        \n",
    "#         posteriors = []\n",
    "#         priors = []\n",
    "#         deters = []\n",
    "#         feats = []\n",
    "#         kl_losses = []\n",
    "        \n",
    "#         for embed, action in zip(embedded_observations, actions):\n",
    "#             posterior, prior, deter = self.observe_step(stoch, deter, action, embed)\n",
    "#             stoch = posterior['sample']\n",
    "            \n",
    "#             posteriors.append(posterior)\n",
    "#             priors.append(prior)\n",
    "#             deters.append(deter)\n",
    "#             feats.append(torch.cat([stoch.flatten(start_dim=1), deter], dim=-1))\n",
    "#             kl_losses.append(kl_loss(posterior['dist'], prior['dist']))\n",
    "\n",
    "#         out = {\n",
    "#             'posteriors': posteriors,\n",
    "#             'priors': priors,\n",
    "#             'deters': torch.swapaxes(torch.stack(deters), 0, 1),\n",
    "#             'feats': torch.swapaxes(torch.stack(feats), 0, 1),\n",
    "#             'kl_losses': torch.swapaxes(torch.stack(kl_losses), 0, 1)\n",
    "#         }\n",
    "         \n",
    "#         return out\n",
    "    \n",
    "# class ImageEncoder(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         Activation = eval(CONFIG.image_encoder.Activation)\n",
    "#         self.N = N = CONFIG.image_encoder.N\n",
    "#         self.color_channels = CONFIG.image.color_channels\n",
    "        \n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(self.color_channels, N*1, 4, 2),\n",
    "#             Activation(),\n",
    "#             nn.Conv2d(32, N*2, 4, 2),\n",
    "#             Activation(),\n",
    "#             nn.Conv2d(64, N*4, 4, 2),\n",
    "#             Activation(),\n",
    "#             nn.Conv2d(128, N*8, 4, 2),\n",
    "#             Activation(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         x = self.model(img)\n",
    "#         return torch.flatten(x, start_dim=1)\n",
    "\n",
    "# class ImageDecoder(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         Activation = eval(CONFIG.image_decoder.Activation)\n",
    "#         self.N = N = CONFIG.image_decoder.N\n",
    "#         self.shape = [\n",
    "#             CONFIG.image.height,\n",
    "#             CONFIG.image.width,\n",
    "#             CONFIG.image.color_channels\n",
    "#         ]\n",
    "#         feat_shape = (\n",
    "#             CONFIG.rssm.stoch_state_classes * CONFIG.rssm.stoch_state_size\n",
    "#             + CONFIG.rssm.det_state_size)\n",
    "        \n",
    "#         self.dense = nn.Linear(feat_shape, N*32)\n",
    "#         self.deconvolve = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(N*32, N*4, 5, 2),\n",
    "#             Activation(),\n",
    "#             nn.ConvTranspose2d(N*4, N*2, 5, 2),\n",
    "#             Activation(),\n",
    "#             nn.ConvTranspose2d(N*2, N, 6, 2),\n",
    "#             Activation(),\n",
    "#             nn.ConvTranspose2d(N, self.shape[-1], 6, 2),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, embed):\n",
    "#         batch_shape = embed.shape[0]\n",
    "#         x = self.dense(embed).reshape(-1, self.N*32, 1, 1)\n",
    "#         x = self.deconvolve(x)\n",
    "#         norm = distributions.Normal(loc=x, scale=1)\n",
    "#         dist = distributions.Independent(norm, len(self.shape))\n",
    "#         assert len(dist.batch_shape) == 1\n",
    "#         assert dist.batch_shape[0] == batch_shape\n",
    "#         return dist\n",
    "\n",
    "# class MLP(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_shape, units, Activation=torch.nn.ELU):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.net = nn.Sequential()\n",
    "        \n",
    "#         for i, unit in enumerate(units):\n",
    "#             self.net.add_module(f\"linear_{i}\", nn.Linear(input_shape, unit))\n",
    "#             self.net.add_module(f\"activation_{i}\", Activation())\n",
    "#             input_shape = unit\n",
    "    \n",
    "#         self.net.add_module(\"out_layer\", nn.Linear(input_shape, 1))\n",
    "            \n",
    "#     def forward(self, features):\n",
    "#         return self.net(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37111ddb-0d2f-436e-a9b7-ac8fbfc0db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.distributions import kl_divergence, Independent\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "from dotmap import DotMap\n",
    "yaml = YAML()\n",
    "with open('./config.yaml', 'r') as f:\n",
    "    CONFIG = DotMap(yaml.load(f))\n",
    "\n",
    "\n",
    "def categorical_kl(probs_a, probs_b):\n",
    "    return torch.sum(probs_a * torch.log(probs_a / probs_b), dim=[-1, -2])\n",
    "\n",
    "\n",
    "def kl_loss(posterior, prior):\n",
    "    lhs = categorical_kl(posterior.probs.detach(), prior.probs)\n",
    "    rhs = categorical_kl(posterior.probs, prior.probs.detach())\n",
    "    kl_loss = CONFIG.rssm.alpha * lhs + (1 - CONFIG.rssm.alpha) * rhs\n",
    "\n",
    "    assert torch.isclose(lhs, rhs).all()\n",
    "\n",
    "    expected = kl_divergence(\n",
    "        Independent(posterior, 1),\n",
    "        Independent(prior, 1),)\n",
    "\n",
    "    assert torch.isclose(lhs, expected, atol=1e-5).all(), lhs - expected\n",
    "\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "class WorldModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env_spec):\n",
    "        super().__init__()\n",
    "        self.env_spec = env_spec\n",
    "\n",
    "        self.rssm = RSSM(action_size=self.env_spec.action_space.n)\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.image_decoder = ImageDecoder()\n",
    "\n",
    "        self.feat_size = (\n",
    "            CONFIG.rssm.stoch_state_classes * CONFIG.rssm.stoch_state_size\n",
    "            + CONFIG.rssm.det_state_size\n",
    "        )\n",
    "\n",
    "        self.reward_predictor = MLP(\n",
    "            input_shape=self.feat_size,\n",
    "            units=CONFIG.reward_head.units,\n",
    "            dist='mse')\n",
    "\n",
    "        self.discount_predictor = MLP(\n",
    "            input_shape=self.feat_size,\n",
    "            units=CONFIG.discount_head.units,\n",
    "            dist='bernoulli')\n",
    "\n",
    "    def forward(self, observations, actions):\n",
    "        segs, steps, channels, height, width = observations.shape\n",
    "        flattened_observations = observations.reshape(\n",
    "            segs*steps, channels, height, width)\n",
    "        embedded_observations = self.image_encoder(\n",
    "            flattened_observations).reshape(segs, steps, -1)\n",
    "        out = self.rssm.observe(embedded_observations, actions)\n",
    "        out['reward_dist'] = self.reward_predictor(out['feats'])\n",
    "        out['discount_dist'] = self.discount_predictor(out['feats'])\n",
    "        flattened_feats = out['feats'].reshape(segs*steps, self.feat_size)\n",
    "        mean = self.image_decoder(flattened_feats).reshape(\n",
    "            segs, steps, channels, height, width)\n",
    "        norm = distributions.Normal(loc=mean, scale=1)\n",
    "        image_recon_dist = distributions.Independent(norm, 3)\n",
    "        assert image_recon_dist.batch_shape == (segs, steps)\n",
    "        out['image_recon_dist'] = image_recon_dist\n",
    "        return out\n",
    "\n",
    "    def loss(self, out, observation_batch, reward_batch, discount_batch):\n",
    "        kl_loss = out['kl_losses'].mean()\n",
    "        reward_loss = -out['reward_dist'].log_prob(reward_batch).mean()\n",
    "        discount_loss = -out['discount_dist'].log_prob(discount_batch).mean()\n",
    "        recon_loss = -out['image_recon_dist'].log_prob(observation_batch).mean()\n",
    "        loss = reward_loss + discount_loss + recon_loss + CONFIG.rssm.beta * kl_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RSSM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size,):\n",
    "        super().__init__()\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.embed_size = CONFIG.rssm.embed_size\n",
    "        self.stoch_state_classes = CONFIG.rssm.stoch_state_classes\n",
    "        self.stoch_state_size = CONFIG.rssm.stoch_state_size\n",
    "        self.det_state_size = CONFIG.rssm.det_state_size\n",
    "        self.act = eval(CONFIG.rssm.act)\n",
    "\n",
    "        self.register_parameter(\n",
    "            name='cell_initial_state',\n",
    "            param=nn.Parameter(torch.zeros(self.det_state_size))\n",
    "        )\n",
    "\n",
    "        self.cell = nn.GRUCell(input_size=self.det_state_size,\n",
    "                               hidden_size=self.det_state_size)\n",
    "\n",
    "        self._initialize_imagination_layers()\n",
    "        self._initialize_observation_layers()\n",
    "\n",
    "    def _initialize_imagination_layers(self):\n",
    "        self.embed_stoch_state_and_action = nn.Linear(\n",
    "            self.action_size + self.stoch_state_size * self.stoch_state_classes,\n",
    "            self.det_state_size)\n",
    "\n",
    "        self.imagine_out_1 = nn.Linear(self.det_state_size, self.det_state_size)\n",
    "        self.imagine_out_2 = nn.Linear(\n",
    "            self.det_state_size,\n",
    "            self.stoch_state_classes*self.stoch_state_size)\n",
    "\n",
    "    def _initialize_observation_layers(self):\n",
    "        self.observe_out_1 = nn.Linear(\n",
    "            self.det_state_size + self.embed_size,\n",
    "            self.det_state_size)\n",
    "        self.observe_out_2 = nn.Linear(\n",
    "            self.det_state_size,\n",
    "            self.stoch_state_classes*self.stoch_state_size)\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = {\n",
    "            'logits': torch.zeros(batch_size,\n",
    "                                  self.stoch_state_size,\n",
    "                                  self.stoch_state_classes),\n",
    "            'stoch': torch.zeros(batch_size,\n",
    "                                 self.stoch_state_size,\n",
    "                                 self.stoch_state_classes),\n",
    "            'deter': self.cell_initial_state.repeat([batch_size, 1])\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def step(self, prev_stoch, prev_deter, prev_action):\n",
    "        x = torch.cat((prev_stoch.flatten(start_dim=1), prev_action), dim=-1)\n",
    "        x = self.act(self.embed_stoch_state_and_action(x))\n",
    "        deter = self.cell(x, prev_deter)\n",
    "        return deter\n",
    "\n",
    "    def get_stoch(self, x):\n",
    "        logits = x.reshape(\n",
    "            *x.shape[:-1],\n",
    "            self.stoch_state_size,\n",
    "            self.stoch_state_classes)\n",
    "        dist = distributions.Categorical(logits=logits)\n",
    "        sample = F.one_hot(dist.sample(), num_classes=self.stoch_state_classes).type(torch.float)\n",
    "        sample += dist.probs - dist.probs.detach()  # Straight through gradients trick\n",
    "        return sample, dist\n",
    "\n",
    "    def imagine_step(self, prev_stoch, prev_deter, prev_action):\n",
    "        deter = self.step(prev_stoch, prev_deter, prev_action)\n",
    "        x = self.act(self.imagine_out_1(deter))\n",
    "        x = self.imagine_out_2(x)\n",
    "        sample, dist = self.get_stoch(x)\n",
    "        prior = {'sample': sample, 'dist': dist}\n",
    "        return prior, deter\n",
    "\n",
    "    def observe_step(self, prev_stoch, prev_deter, prev_action, embed):\n",
    "        prior, deter = self.imagine_step(prev_stoch, prev_deter, prev_action)\n",
    "        x = torch.cat([deter, embed], dim=-1)\n",
    "        x = self.act(self.observe_out_1(x))\n",
    "        x = self.observe_out_2(x)\n",
    "        sample, dist = self.get_stoch(x)\n",
    "        posterior = {'sample': sample, 'dist': dist}\n",
    "        return posterior, prior, deter\n",
    "\n",
    "    def imagine(self):\n",
    "        pass\n",
    "\n",
    "    def observe(self, embedded_observations, actions):\n",
    "        segs, steps, embedding_size = embedded_observations.shape\n",
    "        assert segs == actions.shape[0]\n",
    "        assert steps == actions.shape[1]\n",
    "\n",
    "        # Change from SEGS x STEPS x N -> STEPS x SEGS x N\n",
    "        # This facilitates \n",
    "        embedded_observations = torch.swapaxes(embedded_observations, 0, 1)\n",
    "        actions = torch.swapaxes(actions, 0, 1)\n",
    "\n",
    "        initial = self.initial_state(batch_size=segs)\n",
    "        stoch, deter = initial['stoch'], initial['deter']\n",
    "\n",
    "        posteriors = []\n",
    "        priors = []\n",
    "        deters = []\n",
    "        feats = []\n",
    "        kl_losses = []\n",
    "\n",
    "        for embed, action in zip(embedded_observations, actions):\n",
    "            posterior, prior, deter = self.observe_step(stoch, deter, action, embed)\n",
    "            stoch = posterior['sample']\n",
    "\n",
    "            posteriors.append(posterior)\n",
    "            priors.append(prior)\n",
    "            deters.append(deter)\n",
    "            feats.append(torch.cat([stoch.flatten(start_dim=1), deter], dim=-1))\n",
    "            kl_losses.append(kl_loss(posterior['dist'], prior['dist']))\n",
    "\n",
    "        out = {\n",
    "            'posteriors': posteriors,\n",
    "            'priors': priors,\n",
    "            'deters': torch.swapaxes(torch.stack(deters), 0, 1),\n",
    "            'feats': torch.swapaxes(torch.stack(feats), 0, 1),\n",
    "            'kl_losses': torch.swapaxes(torch.stack(kl_losses), 0, 1)\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        Activation = eval(CONFIG.image_encoder.Activation)\n",
    "        self.N = N = CONFIG.image_encoder.N\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, N*1, 4, 2),\n",
    "            Activation(),\n",
    "            nn.Conv2d(32, N*2, 4, 2),\n",
    "            Activation(),\n",
    "            nn.Conv2d(64, N*4, 4, 2),\n",
    "            Activation(),\n",
    "            nn.Conv2d(128, N*8, 4, 2),\n",
    "            Activation(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.model(img)\n",
    "        return torch.flatten(x, start_dim=1)\n",
    "\n",
    "class ImageDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        Activation = eval(CONFIG.image_decoder.Activation)\n",
    "        self.N = N = CONFIG.image_decoder.N\n",
    "        self.shape = [\n",
    "            CONFIG.image.height,\n",
    "            CONFIG.image.width,\n",
    "            CONFIG.image.color_channels\n",
    "        ]\n",
    "        feat_shape = (\n",
    "            CONFIG.rssm.stoch_state_classes * CONFIG.rssm.stoch_state_size\n",
    "            + CONFIG.rssm.det_state_size)\n",
    "\n",
    "        self.dense = nn.Linear(feat_shape, N*32)\n",
    "        self.deconvolve = nn.Sequential(\n",
    "            nn.ConvTranspose2d(N*32, N*4, 5, 2),\n",
    "            Activation(),\n",
    "            nn.ConvTranspose2d(N*4, N*2, 5, 2),\n",
    "            Activation(),\n",
    "            nn.ConvTranspose2d(N*2, N, 6, 2),\n",
    "            Activation(),\n",
    "            nn.ConvTranspose2d(N, self.shape[-1], 6, 2),\n",
    "            nn.Sigmoid(),  # TODO: Check this\n",
    "        )\n",
    "\n",
    "    def forward(self, embed):\n",
    "        batch_shape = embed.shape[0]\n",
    "        x = self.dense(embed).reshape(-1, self.N*32, 1, 1)\n",
    "        x = self.deconvolve(x)\n",
    "        return x\n",
    "        assert len(dist.batch_shape) == 1\n",
    "        assert dist.batch_shape[0] == batch_shape\n",
    "        return dist\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, units, dist='mse', Activation=torch.nn.ELU):\n",
    "        super().__init__()\n",
    "        self.dist = dist\n",
    "\n",
    "        self.net = nn.Sequential()\n",
    "        for i, unit in enumerate(units):\n",
    "            self.net.add_module(f\"linear_{i}\", nn.Linear(input_shape, unit))\n",
    "            self.net.add_module(f\"activation_{i}\", Activation())\n",
    "            input_shape = unit\n",
    "        self.net.add_module(\"out_layer\", nn.Linear(input_shape, 1))\n",
    "\n",
    "    def forward(self, features):\n",
    "        logits = self.net(features).squeeze()\n",
    "        if self.dist == 'mse':\n",
    "            return torch.distributions.Normal(loc=logits, scale=1)\n",
    "        elif self.dist == 'bernoulli':\n",
    "            return torch.distributions.Bernoulli(logits=logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7520294a-2be6-42e9-8468-cdaee430aa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "924c6db9-c0b0-44c1-a8f8-2f34c9134614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rssm = RSSM(action_size=1)\n",
    "encoder = ImageEncoder()\n",
    "segs = dreamer.buffer.sample_segments(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "18504c56-4f7c-41a5-bd28-5c43db9fbd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-baa315bf8157>:4: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  discount_batch = 1 - torch.tensor([seg.terminals for seg in segs]).type(torch.float)\n"
     ]
    }
   ],
   "source": [
    "observation_batch = torch.tensor([seg.next_observations for seg in segs]).type(torch.float)\n",
    "action_batch = torch.tensor([env.spec.action_space.flatten_n(seg.actions) for seg in segs]).type(torch.float)\n",
    "reward_batch = torch.tensor([seg.rewards for seg in segs]).type(torch.float)\n",
    "discount_batch = 1 - torch.tensor([seg.terminals for seg in segs]).type(torch.float)\n",
    "\n",
    "observation_batch = observation_batch.unsqueeze(2)\n",
    "observation_batch = observation_batch / 255 - 0.5\n",
    "# segs, batch, channels, height, width = observation_batch.shape\n",
    "# flattened_observation_batch = observation_batch.reshape(segs*batch, channels, height, width)\n",
    "# embedded_observations = encoder(flattened_observation_batch).reshape(segs, batch, -1)\n",
    "# embedded_observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9688da68-3a6f-4913-b526-f4c21aaac5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 1, 64, 64])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aef48541-dc24-4f2b-bb5b-9f1b8892a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_model = WorldModel(env.spec)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(world_model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6c5f6aa6-4de3-419f-84a1-163c44615896",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "out = world_model(observation_batch, action_batch)\n",
    "loss = world_model.loss(out, observation_batch, reward_batch, discount_batch)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3a94aae-7a41-46b7-9059-b646b496986f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-92888003d243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/envs/garage_2021.03/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "dist = torch.distributions.Normal(loc=torch.tensor(1.).to('cuda'), scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11505b25-6494-4277-9757-58865e9464d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ImageDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f02424b8-3815-4ec9-98d6-9f86c2dc8c75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-b0f2357257c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feats'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "out['feats'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1adf06df-9f4a-466c-a42d-f264c05c8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_feat_batch = out['feats'].reshape(segs*batch, -1)\n",
    "dist = decoder(flattened_feat_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26ab1d53-8385-4a0d-9bd6-8d935ada1916",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape(): argument 'input' (position 1) must be Tensor, not Independent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-d200990374e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: reshape(): argument 'input' (position 1) must be Tensor, not Independent"
     ]
    }
   ],
   "source": [
    "torch.reshape(dist, (3, 50, 1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c76903a-057b-4de7-849e-71d6dd877aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 1, 64, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e70cf7f-5752-4f9b-a9da-17474c9bac3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "edd2f40d-b82c-492a-b326-4877cdd0f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Normal(loc=out['reward_preds'].squeeze(), scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "354a14f1-d6d9-48fa-bee5-bd28de42aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Bernoulli(logits=out['discount_preds'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7f7046a4-c9f6-4ecb-bb91-76e0573c1a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['kl_losses'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48508d07-2b36-4427-b55f-1248537155c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['reward_preds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80a1e396-cc00-45a8-91f6-aaf9a82c5bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Independent(Normal(loc: torch.Size([150, 1, 64, 64]), scale: torch.Size([150, 1, 64, 64])), 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "431e3ba1-a8f8-4e3a-a281-d26966e0d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " {})"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d26a2f8f-c30a-453f-9c7f-ef621cbe34c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvStep(env_spec=EnvSpec(input_space=Discrete(4), output_space=Box(64, 64), max_episode_length=27000.0), action=1, reward=0.0, observation=array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), env_info={'ale.lives': 5, 'TimeLimit.truncated': False, 'GymEnv.TimeLimitTerminated': False}, step_type=<StepType.FIRST: 0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bfbfee1c-b01c-4b3d-8c57-43dfbcd640ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.0000],\n",
       "        [0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900],\n",
       "        [0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900],\n",
       "        [0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.0000],\n",
       "        [0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900,\n",
       "         0.9900, 0.9900, 0.9900]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ceb5efe-2fd3-42a0-ab46-11c49533126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9189, -0.9190, -0.9190, -0.9189, -0.9190, -0.9190, -0.9190, -0.9189,\n",
       "         -0.9190, -0.9190, -0.9189, -0.9193, -0.9190, -0.9190, -0.9189, -0.9189,\n",
       "         -0.9189, -0.9190, -0.9190, -0.9195, -0.9189, -0.9193, -0.9190, -0.9189,\n",
       "         -0.9189, -0.9190, -0.9189, -0.9190, -0.9190, -0.9190],\n",
       "        [-0.9189, -0.9190, -0.9190, -0.9190, -0.9189, -0.9190, -0.9189, -0.9190,\n",
       "         -0.9189, -0.9193, -0.9189, -0.9191, -0.9190, -0.9189, -0.9190, -0.9190,\n",
       "         -0.9189, -0.9190, -0.9189, -0.9189, -0.9190, -0.9189, -0.9190, -0.9189,\n",
       "         -0.9190, -1.4340, -0.9189, -0.9191, -0.9190, -0.9189],\n",
       "        [-0.9190, -0.9190, -0.9189, -0.9190, -0.9192, -0.9190, -0.9189, -0.9189,\n",
       "         -0.9190, -0.9190, -0.9190, -0.9190, -0.9194, -0.9189, -0.9189, -0.9189,\n",
       "         -0.9190, -0.9190, -0.9190, -0.9190, -0.9189, -0.9191, -0.9190, -0.9191,\n",
       "         -0.9190, -0.9189, -0.9190, -0.9189, -0.9189, -0.9190],\n",
       "        [-0.9189, -0.9190, -0.9190, -0.9189, -0.9190, -0.9189, -0.9190, -0.9189,\n",
       "         -0.9190, -0.9189, -0.9190, -0.9190, -0.9190, -0.9190, -0.9191, -0.9189,\n",
       "         -0.9189, -0.9189, -0.9191, -0.9189, -0.9190, -0.9190, -0.9189, -0.9189,\n",
       "         -0.9190, -0.9192, -0.9189, -0.9190, -0.9189, -0.9190],\n",
       "        [-0.9190, -0.9190, -0.9189, -0.9190, -0.9191, -0.9189, -0.9190, -0.9189,\n",
       "         -0.9190, -0.9191, -0.9191, -0.9193, -0.9190, -0.9190, -0.9191, -0.9190,\n",
       "         -0.9189, -0.9189, -0.9190, -0.9190, -0.9189, -0.9191, -0.9190, -0.9189,\n",
       "         -0.9189, -0.9190, -0.9190, -0.9189, -0.9189, -0.9189]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(reward_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d208a027-e009-4f4a-8957-d506f5cb1a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0e895-5965-447c-a743-a9bc6e8b6abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
